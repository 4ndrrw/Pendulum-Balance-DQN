{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.17.3 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from gym==0.17.3) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from gym==0.17.3) (1.24.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from gym==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from gym==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: tensorflow==2.10 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (24.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (1.17.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\65882\\anaconda3\\envs\\rl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.17.3\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db404f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972e433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6372242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the parameters like gamma, epsilon, epsilon_min, epsilon_decay, learning_rate, alpha, etc. are they all correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc71830",
   "metadata": {},
   "source": [
    "#baseline agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488494d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "        self.gamma = 0.99           # Discount factor -----------------ASK CHER\n",
    "        self.epsilon = 1.0          # Exploration rate \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))  # Q-values for each discrete action\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state[np.newaxis, :], verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "    \n",
    "        states = np.zeros((batch_size, self.state_size))\n",
    "        targets = np.zeros((batch_size, self.action_size))\n",
    "    \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis, :], verbose=0)[0])\n",
    "            target_f = self.model.predict(state[np.newaxis, :], verbose=0)[0]\n",
    "            target_f[action] = target\n",
    "    \n",
    "            states[i] = state\n",
    "            targets[i] = target_f\n",
    "    \n",
    "        \n",
    "        self.GTfit(states, targets)\n",
    "    \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def GTfit(self, X, Y):\n",
    "        X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(X, training=True)\n",
    "            loss = tf.reduce_mean(tf.square(Y - predictions))  # MSE loss\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049a8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env , action_bins, episodes=500, batch_size=64):\n",
    "    reward_history = []\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for time in range(200):\n",
    "            action_idx = agent.act(state)\n",
    "            action = [action_bins[action_idx]]  # env expects an array\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        # ðŸ“Š Cumulative average (all episodes so far)\n",
    "        cumulative_avg = sum(reward_history) / len(reward_history)\n",
    "\n",
    "        # ðŸ“ˆ Moving average (last 5 episodes)\n",
    "        if len(reward_history) >= 5:\n",
    "            moving_avg = sum(reward_history[-5:]) / 5\n",
    "        else:\n",
    "            moving_avg = cumulative_avg  # use current average if < 5 episodes\n",
    "\n",
    "        print(f\"Episode {e+1}: Reward = {total_reward:.2f}, \"\n",
    "              f\"Avg = {cumulative_avg:.2f}, \"\n",
    "              f\"Moving Avg (last 5) = {moving_avg:.2f}\")\n",
    "\n",
    "\n",
    "        if len(reward_history) >= 5:\n",
    "            avg_reward = sum(reward_history[-5:]) / 5\n",
    "            if avg_reward >= -200:\n",
    "                print(f\"âœ… Stopping early: 5-ep avg reward = {avg_reward:.2f} at episode {e+1}\")\n",
    "                break\n",
    "\n",
    "    return reward_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096495db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.  0.  2.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')  # gym 0.17.3\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "NUM_ACTIONS = 3 \n",
    "action_bins = np.linspace(-2, 2, NUM_ACTIONS) \n",
    "print(action_bins)\n",
    "action_size = NUM_ACTIONS  # number of discrete actions\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "#reward_history=train(agent, env, action_bins, episodes=100, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296b4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_rewards(reward_history, moving_avg_window=10, target_reward=-200, title='DQN Training Performance'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot raw episode rewards\n",
    "    plt.plot(reward_history, label='Episode Reward')\n",
    "\n",
    "    # Plot moving average\n",
    "    if len(reward_history) >= moving_avg_window:\n",
    "        moving_avg = np.convolve(reward_history, np.ones(moving_avg_window)/moving_avg_window, mode='valid')\n",
    "        plt.plot(range(moving_avg_window - 1, len(reward_history)), moving_avg, label=f'Moving Avg ({moving_avg_window})')\n",
    "\n",
    "    # Target line\n",
    "    plt.axhline(y=target_reward, color='r', linestyle='--', label=f'Target Reward ({target_reward})')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12f72c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_rewards(reward_history, moving_avg_window=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085bb9a",
   "metadata": {},
   "source": [
    "#improved agent- btr hyperparams and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ed5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class improvedDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99                # discount factor\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.985\n",
    "        self.learning_rate = 0.0005\n",
    "        self.target_update_freq = 10     # update target model every N episodes\n",
    "        self.train_counter = 0\n",
    "\n",
    "        # Build models\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.zeros((batch_size, self.state_size))\n",
    "        targets = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            states[i] = state\n",
    "            target = self.model.predict(state[np.newaxis], verbose=0)[0]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                next_qs = self.target_model.predict(next_state[np.newaxis], verbose=0)[0]\n",
    "                target[action] = reward + self.gamma * np.amax(next_qs)\n",
    "            targets[i] = target\n",
    "\n",
    "        # Use GradientTape to apply custom training step\n",
    "        states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        targets_tf = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(states_tf, training=True)\n",
    "            loss = tf.keras.losses.MSE(targets_tf, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Update target model periodically\n",
    "        self.train_counter += 1\n",
    "        if self.train_counter % self.target_update_freq == 0:\n",
    "            self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df5fa91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.    -1.875 -1.75  -1.625 -1.5   -1.375 -1.25  -1.125 -1.    -0.875\n",
      " -0.75  -0.625 -0.5   -0.375 -0.25  -0.125  0.     0.125  0.25   0.375\n",
      "  0.5    0.625  0.75   0.875  1.     1.125  1.25   1.375  1.5    1.625\n",
      "  1.75   1.875  2.   ]\n"
     ]
    }
   ],
   "source": [
    "NUM_ACTIONS = 33\n",
    "action_bins = np.linspace(-2, 2, NUM_ACTIONS)\n",
    "print(action_bins)\n",
    "action_size = NUM_ACTIONS  # number of discrete actions\n",
    "\n",
    "agent = improvedDQNAgent(state_size, action_size)\n",
    "\n",
    "#reward_history=train(agent, env, action_bins, episodes=250, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad049bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_rewards(reward_history, moving_avg_window=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72ab7fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tunedDQNAgent(improvedDQNAgent):\n",
    "    def __init__(self, state_size, action_size, tau=0.0001):\n",
    "        super().__init__(state_size, action_size)\n",
    "        \n",
    "        # Override hyperparameters\n",
    "        self.learning_rate = 0.0005\n",
    "        self.epsilon_decay = 0.985\n",
    "        self.tau = tau  # soft update factor\n",
    "\n",
    "        # Rebuild model with new architecture and LR\n",
    "        self.model = self._build_tuned_model()\n",
    "        self.target_model = self._build_tuned_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_tuned_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def soft_update_target_model(self):\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = [(1 - self.tau) * tw + self.tau * mw\n",
    "                       for mw, tw in zip(model_weights, target_weights)]\n",
    "        self.target_model.set_weights(new_weights)\n",
    "\n",
    "    def replay(self, batch_size=256):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.zeros((batch_size, self.state_size))\n",
    "        targets = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            states[i] = state\n",
    "            target = self.model.predict(state[np.newaxis], verbose=0)[0]\n",
    "\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                # Double DQN optional here; currently using basic max-Q\n",
    "                next_qs = self.target_model.predict(next_state[np.newaxis], verbose=0)[0]\n",
    "                target[action] = reward + self.gamma * np.amax(next_qs)\n",
    "\n",
    "            targets[i] = target\n",
    "\n",
    "        # Train with GradientTape\n",
    "        states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        targets_tf = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(states_tf, training=True)\n",
    "            loss = tf.keras.losses.MSE(targets_tf, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "\n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Soft update target model\n",
    "        self.soft_update_target_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -888.39, Avg = -888.39, Moving Avg (last 5) = -888.39\n",
      "Episode 2: Reward = -1250.32, Avg = -1069.35, Moving Avg (last 5) = -1069.35\n",
      "Episode 3: Reward = -1511.99, Avg = -1216.90, Moving Avg (last 5) = -1216.90\n",
      "Episode 4: Reward = -1255.80, Avg = -1226.62, Moving Avg (last 5) = -1226.62\n",
      "Episode 5: Reward = -1288.20, Avg = -1238.94, Moving Avg (last 5) = -1238.94\n",
      "Episode 6: Reward = -858.32, Avg = -1175.50, Moving Avg (last 5) = -1232.93\n",
      "Episode 7: Reward = -957.68, Avg = -1144.38, Moving Avg (last 5) = -1174.40\n",
      "Episode 8: Reward = -1268.41, Avg = -1159.89, Moving Avg (last 5) = -1125.68\n",
      "Episode 9: Reward = -1435.43, Avg = -1190.50, Moving Avg (last 5) = -1161.61\n",
      "Episode 10: Reward = -988.29, Avg = -1170.28, Moving Avg (last 5) = -1101.62\n",
      "Episode 11: Reward = -1714.81, Avg = -1219.78, Moving Avg (last 5) = -1272.92\n",
      "Episode 12: Reward = -1183.02, Avg = -1216.72, Moving Avg (last 5) = -1317.99\n",
      "Episode 13: Reward = -1468.50, Avg = -1236.09, Moving Avg (last 5) = -1358.01\n",
      "Episode 14: Reward = -1388.59, Avg = -1246.98, Moving Avg (last 5) = -1348.64\n",
      "Episode 15: Reward = -1162.05, Avg = -1241.32, Moving Avg (last 5) = -1383.39\n",
      "Episode 16: Reward = -1774.96, Avg = -1274.67, Moving Avg (last 5) = -1395.42\n",
      "Episode 17: Reward = -1068.43, Avg = -1262.54, Moving Avg (last 5) = -1372.50\n",
      "Episode 18: Reward = -1761.44, Avg = -1290.26, Moving Avg (last 5) = -1431.09\n",
      "Episode 19: Reward = -1583.57, Avg = -1305.69, Moving Avg (last 5) = -1470.09\n",
      "Episode 20: Reward = -1801.57, Avg = -1330.49, Moving Avg (last 5) = -1597.99\n",
      "Episode 21: Reward = -1165.11, Avg = -1322.61, Moving Avg (last 5) = -1476.02\n",
      "Episode 22: Reward = -1154.80, Avg = -1314.98, Moving Avg (last 5) = -1493.30\n",
      "Episode 23: Reward = -1257.19, Avg = -1312.47, Moving Avg (last 5) = -1392.45\n",
      "Episode 24: Reward = -1291.36, Avg = -1311.59, Moving Avg (last 5) = -1334.01\n",
      "Episode 25: Reward = -1313.24, Avg = -1311.66, Moving Avg (last 5) = -1236.34\n",
      "Episode 26: Reward = -1293.86, Avg = -1310.97, Moving Avg (last 5) = -1262.09\n",
      "Episode 27: Reward = -1623.52, Avg = -1322.55, Moving Avg (last 5) = -1355.84\n",
      "Episode 28: Reward = -1407.25, Avg = -1325.57, Moving Avg (last 5) = -1385.85\n",
      "Episode 29: Reward = -1289.48, Avg = -1324.33, Moving Avg (last 5) = -1385.47\n",
      "Episode 30: Reward = -1416.73, Avg = -1327.41, Moving Avg (last 5) = -1406.17\n",
      "Episode 31: Reward = -1707.95, Avg = -1339.69, Moving Avg (last 5) = -1488.99\n",
      "Episode 32: Reward = -1478.37, Avg = -1344.02, Moving Avg (last 5) = -1459.96\n",
      "Episode 33: Reward = -1607.42, Avg = -1352.00, Moving Avg (last 5) = -1499.99\n",
      "Episode 34: Reward = -1807.46, Avg = -1365.40, Moving Avg (last 5) = -1603.59\n",
      "Episode 35: Reward = -1680.22, Avg = -1374.39, Moving Avg (last 5) = -1656.29\n",
      "Episode 36: Reward = -1169.68, Avg = -1368.71, Moving Avg (last 5) = -1548.63\n",
      "Episode 37: Reward = -1890.28, Avg = -1382.80, Moving Avg (last 5) = -1631.01\n",
      "Episode 38: Reward = -1909.60, Avg = -1396.67, Moving Avg (last 5) = -1691.45\n",
      "Episode 39: Reward = -1415.72, Avg = -1397.15, Moving Avg (last 5) = -1613.10\n",
      "Episode 40: Reward = -1614.23, Avg = -1402.58, Moving Avg (last 5) = -1599.90\n",
      "Episode 41: Reward = -1388.10, Avg = -1402.23, Moving Avg (last 5) = -1643.58\n",
      "Episode 42: Reward = -1429.86, Avg = -1402.89, Moving Avg (last 5) = -1551.50\n",
      "Episode 43: Reward = -1595.03, Avg = -1407.35, Moving Avg (last 5) = -1488.59\n",
      "Episode 44: Reward = -1330.15, Avg = -1405.60, Moving Avg (last 5) = -1471.47\n",
      "Episode 45: Reward = -1898.89, Avg = -1416.56, Moving Avg (last 5) = -1528.41\n",
      "Episode 46: Reward = -1871.43, Avg = -1426.45, Moving Avg (last 5) = -1625.07\n",
      "Episode 47: Reward = -1854.67, Avg = -1435.56, Moving Avg (last 5) = -1710.03\n",
      "Episode 48: Reward = -1922.99, Avg = -1445.72, Moving Avg (last 5) = -1775.62\n",
      "Episode 49: Reward = -1521.94, Avg = -1447.27, Moving Avg (last 5) = -1813.98\n",
      "Episode 50: Reward = -1277.78, Avg = -1443.88, Moving Avg (last 5) = -1689.76\n",
      "Episode 51: Reward = -1729.49, Avg = -1449.48, Moving Avg (last 5) = -1661.37\n",
      "Episode 52: Reward = -1551.38, Avg = -1451.44, Moving Avg (last 5) = -1600.71\n",
      "Episode 53: Reward = -1157.14, Avg = -1445.89, Moving Avg (last 5) = -1447.54\n",
      "Episode 54: Reward = -1156.23, Avg = -1440.52, Moving Avg (last 5) = -1374.40\n",
      "Episode 55: Reward = -1278.39, Avg = -1437.58, Moving Avg (last 5) = -1374.53\n",
      "Episode 56: Reward = -1037.47, Avg = -1430.43, Moving Avg (last 5) = -1236.12\n",
      "Episode 57: Reward = -1071.90, Avg = -1424.14, Moving Avg (last 5) = -1140.23\n",
      "Episode 58: Reward = -988.59, Avg = -1416.63, Moving Avg (last 5) = -1106.52\n",
      "Episode 59: Reward = -1078.92, Avg = -1410.91, Moving Avg (last 5) = -1091.06\n",
      "Episode 60: Reward = -718.24, Avg = -1399.36, Moving Avg (last 5) = -979.02\n",
      "Episode 61: Reward = -764.11, Avg = -1388.95, Moving Avg (last 5) = -924.35\n",
      "Episode 62: Reward = -903.66, Avg = -1381.12, Moving Avg (last 5) = -890.70\n",
      "Episode 63: Reward = -1173.36, Avg = -1377.82, Moving Avg (last 5) = -927.66\n",
      "Episode 64: Reward = -754.43, Avg = -1368.08, Moving Avg (last 5) = -862.76\n",
      "Episode 65: Reward = -1094.13, Avg = -1363.87, Moving Avg (last 5) = -937.94\n",
      "Episode 66: Reward = -907.61, Avg = -1356.96, Moving Avg (last 5) = -966.64\n",
      "Episode 67: Reward = -918.04, Avg = -1350.41, Moving Avg (last 5) = -969.52\n",
      "Episode 68: Reward = -1265.04, Avg = -1349.15, Moving Avg (last 5) = -987.85\n",
      "Episode 69: Reward = -1477.88, Avg = -1351.02, Moving Avg (last 5) = -1132.54\n",
      "Episode 70: Reward = -1283.64, Avg = -1350.05, Moving Avg (last 5) = -1170.44\n",
      "Episode 71: Reward = -1089.08, Avg = -1346.38, Moving Avg (last 5) = -1206.74\n",
      "Episode 72: Reward = -1483.61, Avg = -1348.28, Moving Avg (last 5) = -1319.85\n",
      "Episode 73: Reward = -1460.88, Avg = -1349.83, Moving Avg (last 5) = -1359.02\n",
      "Episode 74: Reward = -1356.68, Avg = -1349.92, Moving Avg (last 5) = -1334.78\n",
      "Episode 75: Reward = -1304.14, Avg = -1349.31, Moving Avg (last 5) = -1338.88\n",
      "Episode 76: Reward = -1465.75, Avg = -1350.84, Moving Avg (last 5) = -1414.21\n",
      "Episode 77: Reward = -1053.85, Avg = -1346.98, Moving Avg (last 5) = -1328.26\n",
      "Episode 78: Reward = -1192.87, Avg = -1345.01, Moving Avg (last 5) = -1274.66\n",
      "Episode 79: Reward = -1270.84, Avg = -1344.07, Moving Avg (last 5) = -1257.49\n",
      "Episode 80: Reward = -1327.76, Avg = -1343.86, Moving Avg (last 5) = -1262.21\n",
      "Episode 81: Reward = -1145.85, Avg = -1341.42, Moving Avg (last 5) = -1198.23\n",
      "Episode 82: Reward = -1153.13, Avg = -1339.12, Moving Avg (last 5) = -1218.09\n",
      "Episode 83: Reward = -1365.29, Avg = -1339.44, Moving Avg (last 5) = -1252.57\n",
      "Episode 84: Reward = -1086.21, Avg = -1336.42, Moving Avg (last 5) = -1215.65\n",
      "Episode 85: Reward = -1252.07, Avg = -1335.43, Moving Avg (last 5) = -1200.51\n",
      "Episode 86: Reward = -1158.06, Avg = -1333.37, Moving Avg (last 5) = -1202.95\n",
      "Episode 87: Reward = -1344.62, Avg = -1333.50, Moving Avg (last 5) = -1241.25\n",
      "Episode 88: Reward = -1395.02, Avg = -1334.20, Moving Avg (last 5) = -1247.20\n",
      "Episode 89: Reward = -1404.94, Avg = -1334.99, Moving Avg (last 5) = -1310.94\n",
      "Episode 90: Reward = -1441.44, Avg = -1336.18, Moving Avg (last 5) = -1348.82\n",
      "Episode 91: Reward = -1246.66, Avg = -1335.19, Moving Avg (last 5) = -1366.54\n",
      "Episode 92: Reward = -1437.21, Avg = -1336.30, Moving Avg (last 5) = -1385.06\n",
      "Episode 93: Reward = -1447.58, Avg = -1337.50, Moving Avg (last 5) = -1395.57\n",
      "Episode 94: Reward = -1329.46, Avg = -1337.41, Moving Avg (last 5) = -1380.47\n",
      "Episode 95: Reward = -1444.43, Avg = -1338.54, Moving Avg (last 5) = -1381.07\n",
      "Episode 96: Reward = -1366.17, Avg = -1338.83, Moving Avg (last 5) = -1404.97\n",
      "Episode 97: Reward = -1365.73, Avg = -1339.10, Moving Avg (last 5) = -1390.68\n",
      "Episode 98: Reward = -1368.60, Avg = -1339.40, Moving Avg (last 5) = -1374.88\n",
      "Episode 99: Reward = -1493.57, Avg = -1340.96, Moving Avg (last 5) = -1407.70\n",
      "Episode 100: Reward = -1452.19, Avg = -1342.07, Moving Avg (last 5) = -1409.25\n",
      "Episode 101: Reward = -1472.29, Avg = -1343.36, Moving Avg (last 5) = -1430.48\n",
      "Episode 102: Reward = -1139.27, Avg = -1341.36, Moving Avg (last 5) = -1385.18\n",
      "Episode 103: Reward = -1428.85, Avg = -1342.21, Moving Avg (last 5) = -1397.23\n",
      "Episode 104: Reward = -1104.73, Avg = -1339.93, Moving Avg (last 5) = -1319.47\n",
      "Episode 105: Reward = -1195.52, Avg = -1338.55, Moving Avg (last 5) = -1268.13\n",
      "Episode 106: Reward = -1269.16, Avg = -1337.90, Moving Avg (last 5) = -1227.51\n",
      "Episode 107: Reward = -1191.31, Avg = -1336.53, Moving Avg (last 5) = -1237.91\n",
      "Episode 108: Reward = -1185.28, Avg = -1335.13, Moving Avg (last 5) = -1189.20\n",
      "Episode 109: Reward = -1101.49, Avg = -1332.98, Moving Avg (last 5) = -1188.55\n",
      "Episode 110: Reward = -1069.94, Avg = -1330.59, Moving Avg (last 5) = -1163.43\n",
      "Episode 111: Reward = -1533.59, Avg = -1332.42, Moving Avg (last 5) = -1216.32\n",
      "Episode 112: Reward = -1079.24, Avg = -1330.16, Moving Avg (last 5) = -1193.91\n",
      "Episode 113: Reward = -1521.97, Avg = -1331.86, Moving Avg (last 5) = -1261.25\n",
      "Episode 114: Reward = -1563.17, Avg = -1333.89, Moving Avg (last 5) = -1353.58\n",
      "Episode 115: Reward = -1575.78, Avg = -1335.99, Moving Avg (last 5) = -1454.75\n",
      "Episode 116: Reward = -1574.03, Avg = -1338.04, Moving Avg (last 5) = -1462.84\n",
      "Episode 117: Reward = -1560.34, Avg = -1339.94, Moving Avg (last 5) = -1559.06\n",
      "Episode 118: Reward = -1605.60, Avg = -1342.19, Moving Avg (last 5) = -1575.78\n",
      "Episode 119: Reward = -1609.81, Avg = -1344.44, Moving Avg (last 5) = -1585.11\n",
      "Episode 120: Reward = -1519.72, Avg = -1345.90, Moving Avg (last 5) = -1573.90\n",
      "Episode 121: Reward = -1569.32, Avg = -1347.75, Moving Avg (last 5) = -1572.96\n",
      "Episode 122: Reward = -1446.64, Avg = -1348.56, Moving Avg (last 5) = -1550.22\n",
      "Episode 123: Reward = -1429.65, Avg = -1349.22, Moving Avg (last 5) = -1515.03\n",
      "Episode 124: Reward = -1423.67, Avg = -1349.82, Moving Avg (last 5) = -1477.80\n",
      "Episode 125: Reward = -1476.33, Avg = -1350.83, Moving Avg (last 5) = -1469.12\n",
      "Episode 126: Reward = -1393.59, Avg = -1351.17, Moving Avg (last 5) = -1433.98\n",
      "Episode 127: Reward = -1414.13, Avg = -1351.67, Moving Avg (last 5) = -1427.47\n",
      "Episode 128: Reward = -1329.05, Avg = -1351.49, Moving Avg (last 5) = -1407.35\n",
      "Episode 129: Reward = -1363.73, Avg = -1351.59, Moving Avg (last 5) = -1395.36\n",
      "Episode 130: Reward = -1238.66, Avg = -1350.72, Moving Avg (last 5) = -1347.83\n",
      "Episode 131: Reward = -1197.66, Avg = -1349.55, Moving Avg (last 5) = -1308.64\n",
      "Episode 132: Reward = -1232.75, Avg = -1348.66, Moving Avg (last 5) = -1272.37\n",
      "Episode 133: Reward = -1246.12, Avg = -1347.89, Moving Avg (last 5) = -1255.78\n",
      "Episode 134: Reward = -900.12, Avg = -1344.55, Moving Avg (last 5) = -1163.06\n",
      "Episode 135: Reward = -998.36, Avg = -1341.99, Moving Avg (last 5) = -1115.00\n",
      "Episode 136: Reward = -1167.13, Avg = -1340.70, Moving Avg (last 5) = -1108.90\n",
      "Episode 137: Reward = -1349.69, Avg = -1340.77, Moving Avg (last 5) = -1132.28\n",
      "Episode 138: Reward = -1189.21, Avg = -1339.67, Moving Avg (last 5) = -1120.90\n",
      "Episode 139: Reward = -1030.96, Avg = -1337.45, Moving Avg (last 5) = -1147.07\n",
      "Episode 140: Reward = -1407.48, Avg = -1337.95, Moving Avg (last 5) = -1228.90\n",
      "Episode 141: Reward = -1536.45, Avg = -1339.36, Moving Avg (last 5) = -1302.76\n",
      "Episode 142: Reward = -1326.78, Avg = -1339.27, Moving Avg (last 5) = -1298.18\n",
      "Episode 143: Reward = -1662.69, Avg = -1341.53, Moving Avg (last 5) = -1392.87\n",
      "Episode 144: Reward = -1616.30, Avg = -1343.44, Moving Avg (last 5) = -1509.94\n",
      "Episode 145: Reward = -1785.96, Avg = -1346.49, Moving Avg (last 5) = -1585.63\n",
      "Episode 146: Reward = -1867.22, Avg = -1350.06, Moving Avg (last 5) = -1651.79\n",
      "Episode 147: Reward = -1562.48, Avg = -1351.50, Moving Avg (last 5) = -1698.93\n",
      "Episode 148: Reward = -1231.31, Avg = -1350.69, Moving Avg (last 5) = -1612.65\n",
      "Episode 149: Reward = -1500.52, Avg = -1351.69, Moving Avg (last 5) = -1589.50\n",
      "Episode 150: Reward = -1283.43, Avg = -1351.24, Moving Avg (last 5) = -1488.99\n",
      "Episode 151: Reward = -1912.36, Avg = -1354.96, Moving Avg (last 5) = -1498.02\n",
      "Episode 152: Reward = -1197.34, Avg = -1353.92, Moving Avg (last 5) = -1424.99\n",
      "Episode 153: Reward = -1217.09, Avg = -1353.02, Moving Avg (last 5) = -1422.15\n",
      "Episode 154: Reward = -1522.71, Avg = -1354.13, Moving Avg (last 5) = -1426.59\n",
      "Episode 155: Reward = -1274.03, Avg = -1353.61, Moving Avg (last 5) = -1424.71\n",
      "Episode 156: Reward = -1217.21, Avg = -1352.73, Moving Avg (last 5) = -1285.68\n",
      "Episode 157: Reward = -1198.24, Avg = -1351.75, Moving Avg (last 5) = -1285.85\n",
      "Episode 158: Reward = -1480.80, Avg = -1352.57, Moving Avg (last 5) = -1338.60\n",
      "Episode 159: Reward = -1475.59, Avg = -1353.34, Moving Avg (last 5) = -1329.17\n",
      "Episode 160: Reward = -1192.68, Avg = -1352.34, Moving Avg (last 5) = -1312.90\n",
      "Episode 161: Reward = -1454.94, Avg = -1352.97, Moving Avg (last 5) = -1360.45\n",
      "Episode 162: Reward = -1503.46, Avg = -1353.90, Moving Avg (last 5) = -1421.49\n",
      "Episode 163: Reward = -1805.42, Avg = -1356.67, Moving Avg (last 5) = -1486.42\n",
      "Episode 164: Reward = -1510.66, Avg = -1357.61, Moving Avg (last 5) = -1493.43\n",
      "Episode 165: Reward = -1292.26, Avg = -1357.22, Moving Avg (last 5) = -1513.35\n",
      "Episode 166: Reward = -1314.99, Avg = -1356.96, Moving Avg (last 5) = -1485.36\n",
      "Episode 167: Reward = -1484.79, Avg = -1357.73, Moving Avg (last 5) = -1481.62\n",
      "Episode 168: Reward = -1471.22, Avg = -1358.40, Moving Avg (last 5) = -1414.78\n",
      "Episode 169: Reward = -1306.39, Avg = -1358.09, Moving Avg (last 5) = -1373.93\n",
      "Episode 170: Reward = -1190.91, Avg = -1357.11, Moving Avg (last 5) = -1353.66\n",
      "Episode 171: Reward = -1238.53, Avg = -1356.42, Moving Avg (last 5) = -1338.37\n",
      "Episode 172: Reward = -1275.12, Avg = -1355.95, Moving Avg (last 5) = -1296.43\n",
      "Episode 173: Reward = -1516.83, Avg = -1356.88, Moving Avg (last 5) = -1305.55\n",
      "Episode 174: Reward = -1497.58, Avg = -1357.68, Moving Avg (last 5) = -1343.79\n",
      "Episode 175: Reward = -1482.64, Avg = -1358.40, Moving Avg (last 5) = -1402.14\n",
      "Episode 176: Reward = -1505.83, Avg = -1359.24, Moving Avg (last 5) = -1455.60\n",
      "Episode 177: Reward = -1235.45, Avg = -1358.54, Moving Avg (last 5) = -1447.66\n",
      "Episode 178: Reward = -1512.15, Avg = -1359.40, Moving Avg (last 5) = -1446.73\n",
      "Episode 179: Reward = -1443.17, Avg = -1359.87, Moving Avg (last 5) = -1435.85\n",
      "Episode 180: Reward = -1504.35, Avg = -1360.67, Moving Avg (last 5) = -1440.19\n",
      "Episode 181: Reward = -1184.04, Avg = -1359.69, Moving Avg (last 5) = -1375.83\n",
      "Episode 182: Reward = -1251.11, Avg = -1359.10, Moving Avg (last 5) = -1378.96\n",
      "Episode 183: Reward = -1462.28, Avg = -1359.66, Moving Avg (last 5) = -1368.99\n",
      "Episode 184: Reward = -1568.41, Avg = -1360.80, Moving Avg (last 5) = -1394.04\n",
      "Episode 185: Reward = -1534.28, Avg = -1361.73, Moving Avg (last 5) = -1400.02\n",
      "Episode 186: Reward = -1569.63, Avg = -1362.85, Moving Avg (last 5) = -1477.14\n",
      "Episode 187: Reward = -1269.74, Avg = -1362.35, Moving Avg (last 5) = -1480.87\n",
      "Episode 188: Reward = -1471.85, Avg = -1362.94, Moving Avg (last 5) = -1482.78\n",
      "Episode 189: Reward = -1485.78, Avg = -1363.59, Moving Avg (last 5) = -1466.26\n",
      "Episode 190: Reward = -1202.43, Avg = -1362.74, Moving Avg (last 5) = -1399.89\n",
      "Episode 191: Reward = -1473.38, Avg = -1363.32, Moving Avg (last 5) = -1380.64\n",
      "Episode 192: Reward = -1451.07, Avg = -1363.77, Moving Avg (last 5) = -1416.90\n",
      "Episode 193: Reward = -1486.08, Avg = -1364.41, Moving Avg (last 5) = -1419.75\n",
      "Episode 194: Reward = -1492.68, Avg = -1365.07, Moving Avg (last 5) = -1421.13\n",
      "Episode 195: Reward = -1288.19, Avg = -1364.67, Moving Avg (last 5) = -1438.28\n",
      "Episode 196: Reward = -1459.43, Avg = -1365.16, Moving Avg (last 5) = -1435.49\n",
      "Episode 197: Reward = -1193.36, Avg = -1364.29, Moving Avg (last 5) = -1383.95\n",
      "Episode 198: Reward = -1582.79, Avg = -1365.39, Moving Avg (last 5) = -1403.29\n",
      "Episode 199: Reward = -1279.29, Avg = -1364.96, Moving Avg (last 5) = -1360.61\n",
      "Episode 200: Reward = -1471.88, Avg = -1365.49, Moving Avg (last 5) = -1397.35\n",
      "Episode 201: Reward = -1500.67, Avg = -1366.16, Moving Avg (last 5) = -1405.60\n",
      "Episode 202: Reward = -1196.40, Avg = -1365.32, Moving Avg (last 5) = -1406.21\n",
      "Episode 203: Reward = -1315.35, Avg = -1365.08, Moving Avg (last 5) = -1352.72\n",
      "Episode 204: Reward = -1188.71, Avg = -1364.21, Moving Avg (last 5) = -1334.60\n",
      "Episode 205: Reward = -1470.98, Avg = -1364.73, Moving Avg (last 5) = -1334.42\n",
      "Episode 206: Reward = -1548.23, Avg = -1365.62, Moving Avg (last 5) = -1343.93\n",
      "Episode 207: Reward = -1573.98, Avg = -1366.63, Moving Avg (last 5) = -1419.45\n",
      "Episode 208: Reward = -1501.26, Avg = -1367.28, Moving Avg (last 5) = -1456.63\n",
      "Episode 209: Reward = -1565.73, Avg = -1368.23, Moving Avg (last 5) = -1532.04\n",
      "Episode 210: Reward = -1291.99, Avg = -1367.86, Moving Avg (last 5) = -1496.24\n",
      "Episode 211: Reward = -1493.82, Avg = -1368.46, Moving Avg (last 5) = -1485.36\n",
      "Episode 212: Reward = -1491.69, Avg = -1369.04, Moving Avg (last 5) = -1468.90\n",
      "Episode 213: Reward = -1504.36, Avg = -1369.68, Moving Avg (last 5) = -1469.52\n"
     ]
    }
   ],
   "source": [
    "agent = tunedDQNAgent(state_size, action_size)\n",
    "reward_history = train(agent, env, action_bins, episodes=2000, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57190e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052dd4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
